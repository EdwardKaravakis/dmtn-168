..
  Technote content.

  See https://developer.lsst.io/restructuredtext/style.html
  for a guide to reStructuredText writing.

  Do not put the title, authors or other metadata in this document;
  those are automatically added.

  Use the following syntax for sections:

  Sections
  ========

  and

  Subsections
  -----------

  and

  Subsubsections
  ^^^^^^^^^^^^^^

  To add images, add the image file (png, svg or jpeg preferred) to the
  _static/ directory. The reST syntax for adding the image is

  .. figure:: /_static/filename.ext
     :name: fig-label

     Caption text.

   Run: ``make html`` and ``open _build/html/index.html`` to preview your work.
   See the README at https://github.com/lsst-sqre/lsst-technote-bootstrap or
   this repo's README for more info.

   Feel free to delete this instructional comment.

:tocdepth: 1

.. Please do not modify tocdepth; will be fixed when a new Sphinx theme is shipped.

.. sectnum::

.. TODO: Delete the note below before merging new content to the master branch.

.. note::

   **This technote is not yet published.**

   Notes on running DRP pipelines using PanDA (Production ANd Distributed Analysis system)

.. Add content here.
.. Do not include the document title (it's automatically added from metadata.yaml).
.. .. rubric:: References

.. Make in-text citations with: :cite:`bibkey`.

.. .. bibliography:: local.bib lsstbib/books.bib lsstbib/lsst.bib lsstbib/lsst-dm.bib lsstbib/refs.bib lsstbib/refs_ads.bib
..    :style: lsst_aa

Introduction
============
The PanDA (Production ANd Distributed Analysis) is a workload management system for distributed (GRID
based), Cloud, or HPC computing. PanDA based setups usually include few leveraged components such as:

- PanDA server [PanDA_twiki]_, [PanDA_paper]_
- JEDI system [JEDI_twiki]_
- Harvester [Harvester_slides]_, [Harvester_paper]_
- Pilot [Pilot_paper]_
- iDDS [iDDS_slides]_
- Monitoring [monitoring_paper]_
- CRIC [CRIC_slides]_

All these components have a well-defined scope of functionality and its independent installation allows to manage deeply
customized instances.
For the year 2019 the ATLAS PanDA instance landed 394M computing jobs on 165 computation queues distributed all over
the World. There are more than 3000 users use BigPanDA monitor. PanDA system has been successfully adopted by other
experiments, e.g. COMPASS [compass_paper]_.

In this note we describe processing the HSC-RC2 dataset within the DRP pipeline and PanDA based workload management
setup.

Setup
=====
In this exercise we have intensively used results of both ATLAS Google POC [atlas_google_poc]_ and Rubin Google POC
[rubin_google_poc]_ projects. The following components of the setup were configured specifically for this exercise:

- **Harvester.** A Kubernetes edge service has been configured in the DOMA Harvester instance to handle two clusters
  deployed on the Google Kubernetes Engine. We distinguished all tasks in the workflow by memory requirements into two
  groups: high (16 GB per pod) and conventional memory (4GB per pod). Queues functioned in the PULL mode. This mode
  continuously provided few instances of pilot idling in the GKE pods and periodically checked for new jobs. The number
  of running pilots and, accordingly, number of activated nodes dynamically increased to match the number jobs
  immediately ready submission on the a queue. Details of Harvester configuration for using Google Cloud Console
  described here [harvester_manual]_.
- **Pilot.** It is a generic approach in grid computing. Instead of submitting jobs directly to the grid gatekeepers, 
  pilot factories are used to submit special lightweight jobs referred to here as pilot wrappers that are executed on 
  the worker nodes. The pilot is responsible for pulling the actual payload from the PanDA server and any input files 
  from the Storage Element (SE), executing the payload, uploading the output to the SE, and sending the final job 
  status to the PanDA server. And pilot jobs are started by pilot-starter jobs, which are provided by the Harvester.
  Currently there are many dependencies of ATLAS and CVMFS in the env setup necessary to run the pilot jobs and authenticate 
  them to pull payload jobs from the PanDA server. We are examining/removing those dependencies, and integrate the minimum 
  OSG stack software into a container. The pilot test is debugged on Google GCP with simple commands, then with lsst jobs 
  interactively. If necessary, some tunings would be made in the pilot code. After that, we will test it on the Google 
  kubernetes cluster.
- **iDDS.** This system was used to provide synchronization of payload execution accordingly to the Direct Acyclic Graph
  (DAG) described in the workflow XML files. These workflow files were generated by scripts prepared for the Rubin
  Google POC and includes quantum graph generation, creation of pickle files and saving workflow in XML format.
  On the iDDS part we have implemented a DAG workflow class for supporting an arbitrary dependencies between jobs, an
  exercise specific task definition module and a submitting script. Provided developments are one time commitments and
  could be reused for further processing campaigns.
- **Google Cloud configuration.**  As it was noted above we defined two kubernetes clusters with different type of
  machines: *e2-medium* with 2 shared cores provided at 50% fraction each and 5 GB of memory and *e2-highmem-2*	with 2
  shared cores provided at 100%	fraction and 16 GB of RAM. Disk size for all machines was set to 30 GB with ordinary
  performance type (not SSD). All nodes were preemtible and autoscaling was enabled for both clusters. An additional
  virtual machine of N2 type (8 cores, 32 GB of RAM) hosted Postgres DB server. We have installed Postgres 12.1 and
  and adjusted default parameters to increase threshold for simultaneous connections and more aggressive usage of RAM
  for caching the data. It appeared that this machine configuration exceed needs and was downgraded to reduce
  operational costs. Data was kept on the Google Storage and accessed using S3 protocol. All payload executed in the
  Singularity containers. To reduce execution overhead images were preconverted from the initial Docker format to a
  native Singularity images and uploaded to the Google storage. A project DNS was configured to use internal traffic
  routes when working nodes SW accessed the data, made queries to the DB or downloaded SW images.

Where were additional improvements of JEDI and iDDS core but they were done into the workflow agnostic way. We have used
a custom built Docker image of Rubin software. We performed debugging which required deployment a custom wrapping
scripts this is why we worked with custom built images however we are going to switch them to the official releases.

PanDA Queues o GKE Clusters and GCS Buckets
===========================================

GKE Clusters
------------

In the project of *panda-dev-1a74*, we defined two large kubernetes (GKE) production clusters, **moderatemem** and **highmem**, and one small GKE test cluster, **developmentcluster**. All clusters are deployed using `Terraform <https://learn.hashicorp.com/collections/terraform/gcp-get-started>`_. The deployment details can be found `here <https://github.com/lsst/idf_deploy>`_.

PanDA Queues
------------

There PanDA queues, **DOMA_LSST_GOOGLE_TEST**, **DOMA_LSST_GOOGLE_TEST_HIMEM**, and **DOMA_LSST_DEV**, are generated accordingly by `The harvester server <https://github.com/HSF/harvester>`_, *ai-idds-02.cern.ch*, corresponding to the GKE clusters, **moderatemem**, **highmem**, and **developmentcluster** respectively. 

The queues are configured by the files under `the panda-conf github repo <https://github.com/lsst-dm/panda-conf/tree/master>`_: `panda_queueconfig.json <https://github.com/lsst-dm/panda-conf/blob/master/harvester/panda_queueconfig.json>`_, `kube_job.yaml <https://github.com/lsst-dm/panda-conf/blob/master/harvester/kube_job.yaml>`_ and `job_dev_prp_driver-gcs.yaml <https://github.com/lsst-dm/panda-conf/blob/master/harvester/job_dev_prp_driver-gcs.yaml>`_. 

The json file *panda_queueconfig.json* accounts for the PanDA queue behavior for all PanDA queues on the harvester server. 
The yaml files configure the POD behavior in the GKE clusters, with the yaml file *kube_job.yaml* for the production queues, **DOMA_LSST_GOOGLE_TEST** and **DOMA_LSST_GOOGLE_TEST_HIMEM**, and the yaml file *job_dev_prp_driver-gcs.yaml* for the test queue **DOMA_LSST_DEV**. The yaml files instruct POD: 

- what container image is used.
- what credentials are passed.
- what commands run in the container on the pod.

For the production queues, the commands inside the container are passed to *"bash -c"*::

 whoami;cd /tmp;export ALRB_noGridMW=NO; wget https://storage.googleapis.com/drp-us-central1-containers/pilots_starter_d3.py; chmod 755 ./pilots_starter_d3.py; ./pilots_starter_d3.py || true

It will download `the pilot package <https://github.com/PanDAWMS/pilot2>`_ and start a new pilot job.

For debugging purpose, a POD node can be created independently with a test yaml file. But remember to use a different metadata name, says, *test-job*, in the yaml file. For example::

 kubectl create -f test.yaml
 kubectl get pods -l job-name=test-job
 kubectl exec -it $podName -- /bin/bash 
 
which creates a pod in the job-name of test-job, and enters to the container on that POD to debug, where $podName is the POD name found on the command "*kubectl get pods*".

GCS Buckets
-----------

In the Google Cloud Storage (GCS), we defined two buckets, **drp-us-central1-containers** and **drp-us-central1-logging**, as shown below:

.. figure:: /_static/GCS_Buckets-in-Rubin.jpg
     :name: List of buckets in the project
     
The 3rd bucket in the name of "us.artifacts.*", was automatically created in the Google Cloud Build, to store the build container images.

As the bucket name indicates, the bucket **drp-us-central1-containers** accommodate container image files, the pilot-related files and panda queue confiuration files. The other bucket **drp-us-central1-logging** stores the log files of pilot and payload jobs.

The logging bucket is configured in *Uniform* access mode, allowing public access, and allowing a special service account **gcs-access** with the permission of **roles/storage.legacyBucketWriter** and **roles/storage.legacyObjectReader**. 
The credential json file of this special service account is generated in the following command::

 gcloud iam service-accounts keys create gcs-access.json --iam-account=gcs-access@${projectID}.iam.gserviceaccount.com

Where $projectID is *panda-dev-1a74*.  Then it is passed to the container on the POD nodes via the secret name *gcs-access*, with the environmental variable **GOOGLE_APPLICATION_CREDENTIAL** pointing to the json file.

Job Run Procedure in PanDA
==========================

The PanDA system is overviewed in the following graph:

.. figure:: /_static/PandaSys.png
     :name: PanDA system overview

Job Submission
--------------

The LSST job tasks are submitted to the PanDA server https://ai-idds-01.cern.ch:25443/server/panda through `the bps plugin <https://github.com/lsst/ctrl_bps>`_. Each task could be composed of many payload jobs. The PanDA server registers those tasks in the central database. `The PanDA monitoring page <https://panda-doma.cern.ch/user/>`_ will show the tasks in the status of "registered", as shown below:

.. figure:: /_static/Jobs-registered.jpg
     :name: Registered PanDA jobs

Job Starting
------------

`The harvester server <https://github.com/HSF/harvester>`_, *ai-idds-02.cern.ch*, is continuously querying the PanDA server about the number of jobs to run, then triggers the corresponding GKE cluster to start up the needed POD nodes. At this moment, those tasks/jobs status will be changed into *running*, as shown below:

.. figure:: /_static/Jobs-running.jpg
     :name: Running PanDA jobs

Job Running
-----------

The POD nodes run in the pilot/Rubin container, for example, *us.gcr.io/panda-dev-1a74/centos:7-stack-lsst_distrib-w_2021_21_osg_d3*, as configured in the GKE cluster. Each jobs on the POD nodes starst one pilot job inside the container.
The pilot job will first get the corresponding PanDA queue configuration and the associated storage ddmendpoint (*RSE*) configuration from `the CRIC information system <http://atlas-cric.cern.ch/>`_. 

The pilot job uses the provided job definition in case of **PUSH** mode, or will get job definition in case of **PULL** mode. 
Then the pilot job runs the provided payload job. In case of **PULL** mode, one pilot job could get and run multiple payload jobs one by one. 
After the payload job finishes, the pilot will use `the pilot client for GCS <https://googleapis.dev/python/storage/latest/index.html>`_ to write the payload job log file into `the Google Cloud Storage bucket <https://storage.googleapis.com/drp-us-central1-logging/>`_, which is defined in the PanDA queue and RSE configuration. 
Then the pilot will update the job status including the public access URL to the log files, as shown below:

.. figure:: /_static/Jobs-done.jpg
     :name: Finished PanDA jobs

If the jobs have not finished successfully, the job status would be *failed*.

The pilot communication with the PanDA server is authenticated with a valid grid proxy, which is passed to the container through POD. Similarly, a credential json file of the GCS bucket access service account is passed to the container, in order to write/access to the GCS bucket in the python client for the Google Cloud Storage.

Job Monitoring
--------------

Users can visit the PanDA monitoring server, `https://panda-doma.cern.ch/user/ <https://panda-doma.cern.ch/user/>`_, to check the job status. The PanDA server fetches the job information from the central database. The monitoring page first shows the summary of user tasks. Click on the task IDs will go into the details of each task, then click on the number under the job status such as *running*, *finished*, or *failed*, will show the list of jobs in that status. You can check each job details by following *the PanDA ID number*.


Workflow generation
===================

Data Processing
===============

Conclusion
==========

References
==========

.. [PanDA_twiki] PanDA Twiki Page `https://twiki.cern.ch/twiki/bin/view/PanDA/PanDA <https://twiki.cern.ch/twiki/bin/view/PanDA/PanDA>`_
.. [PanDA_paper] Evolution of the ATLAS PanDA workload management system for exascale computational science `<https://www.researchgate.net/publication/274619051_Evolution_of_the_ATLAS_PanDA_workload_management_system_for_exascale_computational_science>`_
.. [JEDI_twiki] JEDI Twiki Page `<https://twiki.cern.ch/twiki/bin/view/PanDA/PandaJEDI>`_
.. [Harvester_slides] Harvester Slides `<http://cds.cern.ch/record/2625435/files/ATL-SOFT-SLIDE-2018-400.pdf>`_
.. [Harvester_paper] Harvester: an edge service harvesting heterogeneous resources for ATLAS `<https://www.epj-conferences.org/articles/epjconf/pdf/2019/19/epjconf_chep2018_03030.pdf>`_
.. [Pilot_paper] The next generation PanDA Pilot for and beyond the ATLAS experiment `<https://cds.cern.ch/record/2648507/files/Fulltext.pdf>`_
.. [iDDS_slides] iDDS slides `<https://indico.cern.ch/event/849155/contributions/3576915/attachments/1917085/3170006/idds_20100927_atlas_sc_week.pdf>`_
.. [monitoring_paper] BigPanDA monitoring paper `<https://inspirehep.net/files/37c79d51eadd0e8ec8e019aef8bbcfd8>`_
.. [CRIC_slides] `<https://indico.cern.ch/event/578991/contributions/2738744/attachments/1538768/2412065/20171011_GDB_CRIC_sameNEC.pdf>`_
.. [compass_paper] `<http://ceur-ws.org/Vol-1787/385-388-paper-67.pdf>`_
.. [atlas_google_poc] `<https://indico.bnl.gov/event/8608/contributions/38034/attachments/28380/43694/HEP_Google_May26_2020.pdf>`_
.. [rubin_google_poc] `<https://dmtn-157.lsst.io/>`_
.. [harvester_manual] `<https://github.com/HSF/harvester/wiki/Google-Kubernetes-Engine-setup-and-useful-commands>`_
